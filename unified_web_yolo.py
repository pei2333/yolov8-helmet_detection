from flask import Flask, render_template, request, jsonify, send_file, send_from_directory
from flask_socketio import SocketIO, emit
import threading
import subprocess
import os
import sys
import json
import time
import yaml
from pathlib import Path
import queue
import signal
import psutil
from datetime import datetime
import re
import cv2
import numpy as np
import base64
from io import BytesIO
from PIL import Image
import torch
from ultralytics import YOLO
import shutil
import tempfile
import zipfile

# Flaskåº”ç”¨é…ç½®
app = Flask(__name__)
app.config['SECRET_KEY'] = 'unified_yolo_web_secret_key_2024'
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MBæœ€å¤§ä¸Šä¼ 
socketio = SocketIO(app, cors_allowed_origins="*", max_size=50*1024*1024)

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# å…¨å±€å˜é‡
training_process = None
current_training_info = {
    'status': 'idle',
    'model': None,
    'epochs': 0,
    'current_epoch': 0,
    'metrics': {},
    'start_time': None,
    'log_buffer': [],
    'process_id': None
}

# æ¨ç†çŠ¶æ€
inference_models = {}
current_inference_model = None

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨"""
    def __init__(self):
        self.process = None
        self.log_queue = queue.Queue()
        
    def start_training(self, model_type, epochs=100, data_path='dataset_OnHands/data.yaml', batch_size=8):
        """å¯åŠ¨è®­ç»ƒè¿›ç¨‹"""
        global current_training_info
        
        if self.process and self.process.poll() is None:
            return False, "è®­ç»ƒå·²åœ¨è¿›è¡Œä¸­"
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        cmd = [
            sys.executable, 'train_model.py',  # ä½¿ç”¨å½“å‰Pythonè§£é‡Šå™¨
            model_type,
            '--epochs', str(epochs),
            '--data', data_path,
            '--workers', '0',
            '--batch', str(batch_size)
        ]
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['PYTHONPATH'] = os.getcwd()
        env['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
        
        try:
            # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1,
                cwd=os.getcwd(),
                env=env,
                encoding='utf-8',
                errors='ignore'
            )
            
            # æ›´æ–°è®­ç»ƒä¿¡æ¯
            current_training_info.update({
                'status': 'training',
                'model': model_type,
                'epochs': epochs,
                'current_epoch': 0,
                'start_time': datetime.now().isoformat(),
                'process_id': self.process.pid,
                'log_buffer': [],
                'batch_size': batch_size,
                'data_path': data_path
            })
            
            # å¯åŠ¨æ—¥å¿—ç›‘æ§çº¿ç¨‹
            log_thread = threading.Thread(target=self._monitor_logs)
            log_thread.daemon = True
            log_thread.start()
            
            return True, "è®­ç»ƒå¯åŠ¨æˆåŠŸ"
            
        except Exception as e:
            return False, f"å¯åŠ¨è®­ç»ƒå¤±è´¥: {str(e)}"
    
    def stop_training(self):
        """åœæ­¢è®­ç»ƒè¿›ç¨‹"""
        global current_training_info
        
        if self.process and self.process.poll() is None:
            try:
                # å°è¯•ä¼˜é›…å…³é—­
                parent = psutil.Process(self.process.pid)
                children = parent.children(recursive=True)
                for child in children:
                    child.terminate()
                parent.terminate()
                
                # ç­‰å¾…è¿›ç¨‹ç»“æŸ
                gone, alive = psutil.wait_procs([parent] + children, timeout=5)
                for p in alive:
                    p.kill()
                
                current_training_info['status'] = 'stopped'
                socketio.emit('training_stopped', {'message': 'è®­ç»ƒå·²åœæ­¢'})
                return True, "è®­ç»ƒå·²åœæ­¢"
                
            except Exception as e:
                return False, f"åœæ­¢è®­ç»ƒå¤±è´¥: {str(e)}"
        else:
            return False, "æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„è®­ç»ƒè¿›ç¨‹"
    
    def _monitor_logs(self):
        """ç›‘æ§è®­ç»ƒæ—¥å¿—è¾“å‡º"""
        global current_training_info
        
        if not self.process:
            return
        
        try:
            for line in iter(self.process.stdout.readline, ''):
                if line:
                    line = line.strip()
                    current_training_info['log_buffer'].append(line)
                    
                    # ä¿æŒæ—¥å¿—ç¼“å†²åŒºå¤§å°
                    if len(current_training_info['log_buffer']) > 1000:
                        current_training_info['log_buffer'] = current_training_info['log_buffer'][-500:]
                    
                    # è§£æè®­ç»ƒæŒ‡æ ‡
                    self._parse_metrics(line)
                    
                    # å®æ—¶å‘é€æ—¥å¿—
                    socketio.emit('training_log', {'log': line})
            
            # è¿›ç¨‹ç»“æŸ
            self.process.wait()
            if current_training_info['status'] == 'training':
                current_training_info['status'] = 'completed'
                socketio.emit('training_completed', {'message': 'è®­ç»ƒå®Œæˆ'})
                
        except Exception as e:
            current_training_info['status'] = 'error'
            socketio.emit('training_error', {'error': str(e)})
    
    def _parse_metrics(self, log_line):
        """è§£æè®­ç»ƒæŒ‡æ ‡"""
        global current_training_info
        
        # è§£æepochä¿¡æ¯
        epoch_match = re.search(r'Epoch\s+(\d+)/(\d+)', log_line)
        if epoch_match:
            current_epoch = int(epoch_match.group(1))
            total_epochs = int(epoch_match.group(2))
            current_training_info['current_epoch'] = current_epoch
            current_training_info['epochs'] = total_epochs
            
            progress = (current_epoch / total_epochs) * 100
            socketio.emit('training_progress', {
                'epoch': current_epoch,
                'total_epochs': total_epochs,
                'progress': progress
            })
        
        # è§£æmAPæŒ‡æ ‡
        map_match = re.search(r'mAP50:\s*([\d.]+)', log_line)
        if map_match:
            map50 = float(map_match.group(1))
            current_training_info['metrics']['mAP50'] = map50
            socketio.emit('metrics_update', {'mAP50': map50})
        
        # è§£ælossä¿¡æ¯
        loss_match = re.search(r'loss:\s*([\d.]+)', log_line)
        if loss_match:
            loss = float(loss_match.group(1))
            current_training_info['metrics']['loss'] = loss
            socketio.emit('metrics_update', {'loss': loss})

class InferenceEngine:
    """æ¨ç†å¼•æ“"""
    def __init__(self):
        self.models = {}
        self.current_model = None
        
    def load_model(self, model_path, model_name=None):
        """åŠ è½½æ¨¡å‹"""
        try:
            if not Path(model_path).exists():
                return False, f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"
            
            model = YOLO(model_path)
            if model_name is None:
                model_name = Path(model_path).stem
            
            self.models[model_name] = {
                'model': model,
                'path': model_path,
                'loaded_time': datetime.now().isoformat()
            }
            
            return True, f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ"
            
        except Exception as e:
            return False, f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}"
    
    def set_current_model(self, model_name):
        """è®¾ç½®å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
        if model_name in self.models:
            self.current_model = model_name
            return True, f"åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}"
        else:
            return False, f"æ¨¡å‹ {model_name} æœªåŠ è½½"
    
    def predict_image(self, image_data, conf_threshold=0.5):
        """å›¾åƒæ¨ç†"""
        if not self.current_model or self.current_model not in self.models:
            return False, "æ²¡æœ‰é€‰æ‹©æœ‰æ•ˆçš„æ¨¡å‹", None
        
        try:
            model = self.models[self.current_model]['model']
            
            # å¤„ç†base64å›¾åƒæ•°æ®
            if isinstance(image_data, str) and image_data.startswith('data:image'):
                # è§£ç base64å›¾åƒ
                header, data = image_data.split(',', 1)
                image_bytes = base64.b64decode(data)
                image = Image.open(BytesIO(image_bytes))
                image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            else:
                # ç›´æ¥ä½¿ç”¨å›¾åƒè·¯å¾„
                image = cv2.imread(image_data)
            
            if image is None:
                return False, "æ— æ³•è¯»å–å›¾åƒ", None
            
            # æ¨ç†
            results = model(image, conf=conf_threshold)
            
            # å¤„ç†ç»“æœ
            result_data = self._process_results(results[0], image)
            
            return True, "æ¨ç†æˆåŠŸ", result_data
            
        except Exception as e:
            return False, f"æ¨ç†å¤±è´¥: {str(e)}", None
    
    def _process_results(self, result, original_image):
        """å¤„ç†æ¨ç†ç»“æœ"""
        try:
            # ç»˜åˆ¶æ£€æµ‹æ¡†
            annotated_image = result.plot()
            
            # è½¬æ¢ä¸ºbase64
            _, buffer = cv2.imencode('.jpg', annotated_image)
            image_base64 = base64.b64encode(buffer).decode('utf-8')
            
            # æå–æ£€æµ‹ä¿¡æ¯
            detections = []
            if result.boxes is not None:
                boxes = result.boxes.xyxy.cpu().numpy()
                confs = result.boxes.conf.cpu().numpy()
                classes = result.boxes.cls.cpu().numpy()
                
                class_names = {0: "head", 1: "helmet"}
                
                for i, (box, conf, cls) in enumerate(zip(boxes, confs, classes)):
                    x1, y1, x2, y2 = box
                    detections.append({
                        'id': i,
                        'class': class_names.get(int(cls), f"class_{int(cls)}"),
                        'confidence': float(conf),
                        'bbox': [float(x1), float(y1), float(x2), float(y2)],
                        'area': float((x2-x1) * (y2-y1))
                    })
            
            return {
                'image': f"data:image/jpeg;base64,{image_base64}",
                'detections': detections,
                'summary': {
                    'total_detections': len(detections),
                    'head_count': len([d for d in detections if d['class'] == 'head']),
                    'helmet_count': len([d for d in detections if d['class'] == 'helmet']),
                    'safety_rate': len([d for d in detections if d['class'] == 'helmet']) / max(1, len(detections)) * 100
                }
            }
            
        except Exception as e:
            raise Exception(f"å¤„ç†ç»“æœå¤±è´¥: {str(e)}")
    
    def get_model_info(self, model_name):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if model_name not in self.models:
            return None
        
        model_info = self.models[model_name]
        model = model_info['model']
        
        try:
            # è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
            model_stats = {
                'name': model_name,
                'path': model_info['path'],
                'loaded_time': model_info['loaded_time'],
                'task': getattr(model, 'task', 'detect'),
                'names': getattr(model.model, 'names', {}),
                'device': str(next(model.model.parameters()).device) if hasattr(model, 'model') else 'unknown'
            }
            
            # è®¡ç®—æ¨¡å‹å¤§å°
            model_path = Path(model_info['path'])
            if model_path.exists():
                model_stats['file_size'] = model_path.stat().st_size
                model_stats['file_size_mb'] = round(model_stats['file_size'] / 1024 / 1024, 2)
            
            return model_stats
            
        except Exception as e:
            return {'name': model_name, 'error': str(e)}

# åˆ›å»ºå®ä¾‹
monitor = TrainingMonitor()
inference_engine = InferenceEngine()

# ============= è·¯ç”±å®šä¹‰ =============

@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    return render_template('unified_yolo.html')

@app.route('/training')
def training_page():
    """è®­ç»ƒé¡µé¢"""
    return render_template('unified_yolo.html', page='training')

@app.route('/inference')
def inference_page():
    """æ¨ç†é¡µé¢"""
    return render_template('unified_yolo.html', page='inference')

@app.route('/models')
def models_page():
    """æ¨¡å‹ç®¡ç†é¡µé¢"""
    return render_template('unified_yolo.html', page='models')

# ============= è®­ç»ƒAPI =============

@app.route('/api/models')
def get_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    models = [
        {'id': 'baseline', 'name': 'YOLOv8s Baseline', 'description': 'åŸºçº¿æ¨¡å‹', 'category': 'baseline'},
        {'id': 'csp-ctfn', 'name': 'CSP-CTFN Only', 'description': 'CSP-CTFNæ¨¡å—ä¼˜åŒ–', 'category': 'component'},
        {'id': 'psc-head', 'name': 'PSC-Head Only', 'description': 'PSCæ£€æµ‹å¤´ä¼˜åŒ–', 'category': 'component'},
        {'id': 'siou', 'name': 'SIoU Only', 'description': 'SIoUæŸå¤±å‡½æ•°ä¼˜åŒ–', 'category': 'component'},
        {'id': 'lw-yolov8', 'name': 'LW-YOLOv8 Full', 'description': 'å®Œæ•´è½»é‡åŒ–æ¨¡å‹', 'category': 'lightweight'},
        {'id': 'improved-csp-ctfn', 'name': 'Improved CSP-CTFN', 'description': 'æ”¹è¿›çš„CSP-CTFNæ¨¡å—', 'category': 'improved'},
        {'id': 'plus', 'name': 'YOLOv8-PLUS', 'description': 'é›†æˆC3k2+SPPF+C2PSAçš„å¢å¼ºç‰ˆæœ¬', 'category': 'enhanced'}
    ]
    return jsonify(models)

@app.route('/api/datasets')
def get_datasets():
    """è·å–å¯ç”¨æ•°æ®é›†"""
    datasets = []
    dataset_paths = [
        'dataset_OnHands/data.yaml',
        'datasets_mini/dataset_mini.yaml'
    ]
    
    for path in dataset_paths:
        if Path(path).exists():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)
                datasets.append({
                    'path': path,
                    'name': Path(path).stem,
                    'train': data.get('train', ''),
                    'val': data.get('val', ''),
                    'nc': data.get('nc', 0),
                    'names': data.get('names', [])
                })
            except:
                datasets.append({
                    'path': path,
                    'name': Path(path).stem,
                    'error': 'æ— æ³•è¯»å–é…ç½®æ–‡ä»¶'
                })
    
    return jsonify(datasets)

@app.route('/api/start_training', methods=['POST'])
def start_training():
    """å¯åŠ¨è®­ç»ƒ"""
    data = request.json
    model_type = data.get('model', 'baseline')
    epochs = data.get('epochs', 100)
    data_path = data.get('dataset', 'dataset_OnHands/data.yaml')
    batch_size = data.get('batch_size', 8)
    
    success, message = monitor.start_training(model_type, epochs, data_path, batch_size)
    return jsonify({'success': success, 'message': message})

@app.route('/api/stop_training', methods=['POST'])
def stop_training():
    """åœæ­¢è®­ç»ƒ"""
    success, message = monitor.stop_training()
    return jsonify({'success': success, 'message': message})

@app.route('/api/training_status')
def get_training_status():
    """è·å–è®­ç»ƒçŠ¶æ€"""
    return jsonify(current_training_info)

@app.route('/api/runs')
def get_runs():
    """è·å–è®­ç»ƒç»“æœåˆ—è¡¨"""
    runs_dir = Path('runs/train')
    runs = []
    
    if runs_dir.exists():
        for run_dir in runs_dir.iterdir():
            if run_dir.is_dir():
                weights_file = run_dir / 'weights' / 'best.pt'
                results_file = run_dir / 'results.png'
                
                # å°è¯•è¯»å–è®­ç»ƒé…ç½®
                config_info = {}
                try:
                    args_file = run_dir / 'args.yaml'
                    if args_file.exists():
                        with open(args_file, 'r') as f:
                            config_info = yaml.safe_load(f)
                except:
                    pass
                
                run_info = {
                    'name': run_dir.name,
                    'path': str(run_dir),
                    'has_weights': weights_file.exists(),
                    'has_results': results_file.exists(),
                    'created': run_dir.stat().st_mtime,
                    'weights_path': str(weights_file) if weights_file.exists() else None,
                    'results_path': str(results_file) if results_file.exists() else None,
                    'config': config_info
                }
                runs.append(run_info)
    
    # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
    runs.sort(key=lambda x: x['created'], reverse=True)
    return jsonify(runs)

# ============= æ¨ç†API =============

@app.route('/api/inference/models')
def get_inference_models():
    """è·å–å·²åŠ è½½çš„æ¨ç†æ¨¡å‹"""
    models_info = []
    for name, info in inference_engine.models.items():
        model_info = inference_engine.get_model_info(name)
        if model_info:
            model_info['is_current'] = (name == inference_engine.current_model)
            models_info.append(model_info)
    
    return jsonify(models_info)

@app.route('/api/inference/load_model', methods=['POST'])
def load_inference_model():
    """åŠ è½½æ¨ç†æ¨¡å‹"""
    data = request.json
    model_path = data.get('model_path')
    model_name = data.get('model_name')
    
    if not model_path:
        return jsonify({'success': False, 'message': 'è¯·æä¾›æ¨¡å‹è·¯å¾„'})
    
    success, message = inference_engine.load_model(model_path, model_name)
    return jsonify({'success': success, 'message': message})

@app.route('/api/inference/set_model', methods=['POST'])
def set_inference_model():
    """è®¾ç½®å½“å‰æ¨ç†æ¨¡å‹"""
    data = request.json
    model_name = data.get('model_name')
    
    success, message = inference_engine.set_current_model(model_name)
    return jsonify({'success': success, 'message': message})

@app.route('/api/inference/predict', methods=['POST'])
def predict_image():
    """å›¾åƒæ¨ç†"""
    try:
        if 'image' in request.files:
            # æ–‡ä»¶ä¸Šä¼ 
            file = request.files['image']
            if file.filename == '':
                return jsonify({'success': False, 'message': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'})
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_path = os.path.join(tempfile.gettempdir(), f"temp_inference_{int(time.time())}_{file.filename}")
            file.save(temp_path)
            
            conf_threshold = float(request.form.get('conf_threshold', 0.5))
            
            success, message, result_data = inference_engine.predict_image(temp_path, conf_threshold)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(temp_path)
            except:
                pass
            
        else:
            # JSONæ•°æ®
            data = request.json
            image_data = data.get('image_data')
            conf_threshold = data.get('conf_threshold', 0.5)
            
            success, message, result_data = inference_engine.predict_image(image_data, conf_threshold)
        
        if success:
            return jsonify({'success': True, 'message': message, 'result': result_data})
        else:
            return jsonify({'success': False, 'message': message})
            
    except Exception as e:
        return jsonify({'success': False, 'message': f'æ¨ç†å¤±è´¥: {str(e)}'})

# ============= æ¨¡å‹ç®¡ç†API =============

@app.route('/api/models/compare', methods=['POST'])
def compare_models():
    """æ¨¡å‹å¯¹æ¯”"""
    data = request.json
    run_names = data.get('runs', [])
    
    comparison_data = []
    for run_name in run_names:
        run_dir = Path('runs/train') / run_name
        if run_dir.exists():
            # è¯»å–è®­ç»ƒç»“æœ
            results = {}
            try:
                # å°è¯•è¯»å–ç»“æœæ–‡ä»¶
                results_csv = run_dir / 'results.csv'
                if results_csv.exists():
                    import pandas as pd
                    df = pd.read_csv(results_csv)
                    if not df.empty:
                        latest = df.iloc[-1]
                        results = {
                            'mAP50': float(latest.get('metrics/mAP50(B)', 0)),
                            'mAP50-95': float(latest.get('metrics/mAP50-95(B)', 0)),
                            'precision': float(latest.get('metrics/precision(B)', 0)),
                            'recall': float(latest.get('metrics/recall(B)', 0)),
                            'final_epoch': int(latest.get('epoch', 0))
                        }
            except:
                pass
            
            # è·å–æ¨¡å‹å¤§å°
            weights_file = run_dir / 'weights' / 'best.pt'
            model_size = 0
            if weights_file.exists():
                model_size = weights_file.stat().st_size / 1024 / 1024  # MB
            
            comparison_data.append({
                'name': run_name,
                'metrics': results,
                'model_size_mb': round(model_size, 2),
                'weights_path': str(weights_file) if weights_file.exists() else None
            })
    
    return jsonify(comparison_data)

@app.route('/api/models/download/<run_name>')
def download_model(run_name):
    """ä¸‹è½½æ¨¡å‹æƒé‡"""
    run_dir = Path('runs/train') / run_name
    weights_file = run_dir / 'weights' / 'best.pt'
    
    if weights_file.exists():
        return send_file(weights_file, as_attachment=True, download_name=f'{run_name}_best.pt')
    else:
        return jsonify({'error': 'æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨'}), 404

@app.route('/api/models/export', methods=['POST'])
def export_models():
    """å¯¼å‡ºå¤šä¸ªæ¨¡å‹ä¸ºZIPåŒ…"""
    data = request.json
    run_names = data.get('runs', [])
    
    if not run_names:
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ¨¡å‹'}), 400
    
    # åˆ›å»ºä¸´æ—¶ZIPæ–‡ä»¶
    temp_zip = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
    
    try:
        with zipfile.ZipFile(temp_zip.name, 'w') as zipf:
            for run_name in run_names:
                run_dir = Path('runs/train') / run_name
                weights_file = run_dir / 'weights' / 'best.pt'
                
                if weights_file.exists():
                    zipf.write(weights_file, f'{run_name}_best.pt')
                
                # æ·»åŠ ç»“æœå›¾ç‰‡
                results_file = run_dir / 'results.png'
                if results_file.exists():
                    zipf.write(results_file, f'{run_name}_results.png')
        
        return send_file(temp_zip.name, as_attachment=True, download_name='yolo_models.zip')
        
    except Exception as e:
        return jsonify({'error': f'å¯¼å‡ºå¤±è´¥: {str(e)}'}), 500
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å°†åœ¨å“åº”å‘é€åè¿›è¡Œ
        pass

# ============= æ–‡ä»¶æœåŠ¡ =============

@app.route('/results/<path:filename>')
def serve_results(filename):
    """æä¾›ç»“æœæ–‡ä»¶æœåŠ¡"""
    return send_from_directory('runs/train', filename)

# ============= WebSocketå¤„ç† =============

@socketio.on('connect')
def handle_connect():
    """WebSocketè¿æ¥å¤„ç†"""
    emit('connected', {'message': 'è¿æ¥æˆåŠŸ'})
    # å‘é€å½“å‰è®­ç»ƒçŠ¶æ€
    emit('training_status', current_training_info)

@socketio.on('disconnect')
def handle_disconnect():
    """WebSocketæ–­å¼€å¤„ç†"""
    print('å®¢æˆ·ç«¯æ–­å¼€è¿æ¥')

@socketio.on('request_logs')
def handle_request_logs():
    """è¯·æ±‚å†å²æ—¥å¿—"""
    logs = current_training_info.get('log_buffer', [])
    emit('historical_logs', {'logs': logs})

@socketio.on('join_room')
def handle_join_room(data):
    """åŠ å…¥æˆ¿é—´ï¼ˆç”¨äºåˆ†ç»„é€šä¿¡ï¼‰"""
    room = data.get('room', 'default')
    # join_room(room)
    emit('joined_room', {'room': room})

# ============= å·¥å…·å‡½æ•° =============

def ensure_directories():
    """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    dirs = ['templates', 'static', 'runs', 'runs/train', 'uploads', 'temp']
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

def load_available_models():
    """è‡ªåŠ¨åŠ è½½å¯ç”¨çš„æ¨¡å‹"""
    runs_dir = Path('runs/train')
    if runs_dir.exists():
        for run_dir in runs_dir.iterdir():
            if run_dir.is_dir():
                weights_file = run_dir / 'weights' / 'best.pt'
                if weights_file.exists():
                    success, message = inference_engine.load_model(str(weights_file), run_dir.name)
                    if success:
                        print(f"âœ… è‡ªåŠ¨åŠ è½½æ¨¡å‹: {run_dir.name}")

# ============= åº”ç”¨å¯åŠ¨ =============

if __name__ == '__main__':
    # ç¡®ä¿ç›®å½•ç»“æ„
    ensure_directories()
    
    # è‡ªåŠ¨åŠ è½½å¯ç”¨æ¨¡å‹
    load_available_models()
    
    print("ğŸš€ å¯åŠ¨ç»Ÿä¸€YOLOv8 Webåº”ç”¨...")
    print("åŠŸèƒ½åŒ…æ‹¬:")
    print("  ğŸ“ˆ å®æ—¶è®­ç»ƒç›‘æ§")
    print("  ğŸ” æ¨¡å‹æ¨ç†æµ‹è¯•")
    print("  ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("  ğŸ’¾ æ¨¡å‹ç®¡ç†ä¸‹è½½")
    print("è®¿é—®åœ°å€: http://localhost:5000")
    print("=" * 50)
    
    socketio.run(app, host='0.0.0.0', port=5000, debug=False, allow_unsafe_werkzeug=True) 