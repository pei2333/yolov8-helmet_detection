# ğŸš§ è½»é‡åŒ–å®‰å…¨å¸½æ£€æµ‹ç³»ç»Ÿ

åŸºäºYOLOv8çš„è½»é‡åŒ–æ”¹è¿›ï¼Œä¸“é—¨é’ˆå¯¹å®‰å…¨å¸½æ£€æµ‹åœºæ™¯ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿã€‚

## ğŸŒŸ é¡¹ç›®ç‰¹è‰²

### è½»é‡åŒ–æ”¹è¿›æ–¹æ¡ˆ

| æ¨¡å— | æ”¹è¿›æ–¹æ³• | æ•ˆæœ |
|------|----------|------|
| **ğŸ”¥ éª¨å¹²ç½‘ç»œ** | FasterNet Block (PConv + Conv) | å‡å°‘30% C2få‚æ•°é‡ï¼Œä¿æŒç‰¹å¾æå–èƒ½åŠ› |
| **ğŸŒŸ é¢ˆéƒ¨ç½‘ç»œ** | FSDIå…¨è¯­ä¹‰å’Œç»†èŠ‚èåˆ | æé«˜å°ç›®æ ‡æ£€æµ‹ç²¾åº¦2-3% |
| **ğŸ” ç‰¹å¾é‡‘å­—å¡”** | MB-FPNå¤šåˆ†æ”¯ç‰¹å¾é‡‘å­—å¡” | æ”¹å–„å¤šå°ºåº¦ç›®æ ‡æ£€æµ‹æ€§èƒ½ |
| **âš¡ æ£€æµ‹å¤´** | LSCDè½»é‡åŒ–å…±äº«å·ç§¯ | å‡å°‘19%å‚æ•°å’Œ10%è®¡ç®—é‡ |
| **ğŸ“Š æŸå¤±å‡½æ•°** | Focaler-CIOU + Enhanced Focal | è§£å†³æ ·æœ¬ä¸å¹³è¡¡ï¼Œæé«˜éš¾æ ·æœ¬æ£€æµ‹ |

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
safety_helmet_detection/
â”œâ”€â”€ main.py                    # ğŸ® ä¸»æ§è„šæœ¬ï¼ˆäº¤äº’å¼èœå•ï¼‰
â”œâ”€â”€ requirements.txt           # ğŸ“¦ é¡¹ç›®ä¾èµ–
â”œâ”€â”€ README.md                 # ğŸ“– é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ configs/                  # âš™ï¸ é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ safety_helmet.yaml   # æ•°æ®é›†é…ç½®
â”œâ”€â”€ modules/                  # ğŸ§© è½»é‡åŒ–æ¨¡å—
â”‚   â”œâ”€â”€ fasternet.py         # FasterNetè½»é‡åŒ–å·ç§¯
â”‚   â”œâ”€â”€ fsdi.py              # å…¨è¯­ä¹‰å’Œç»†èŠ‚èåˆ
â”‚   â”œâ”€â”€ mb_fpn.py            # å¤šåˆ†æ”¯ç‰¹å¾é‡‘å­—å¡”
â”‚   â”œâ”€â”€ attention.py         # A2åŒºåŸŸæ³¨æ„åŠ› + PAM
â”‚   â”œâ”€â”€ lscd.py              # è½»é‡åŒ–å…±äº«å·ç§¯æ£€æµ‹å¤´
â”‚   â””â”€â”€ losses.py            # Focaler-CIOUæŸå¤±å‡½æ•°
â”œâ”€â”€ models/                  # ğŸ¤– è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ baseline_trainer.py  # åŸºçº¿YOLOv8è®­ç»ƒ
â”‚   â”œâ”€â”€ lightweight_trainer.py  # è½»é‡åŒ–æ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ attention_trainer.py    # æ³¨æ„åŠ›å¢å¼ºè®­ç»ƒ
â”‚   â””â”€â”€ optimized_trainer.py    # å®Œæ•´ä¼˜åŒ–æ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ utils/                   # ğŸ› ï¸ å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ dataset_utils.py     # æ•°æ®é›†å¤„ç†å·¥å…·
â”œâ”€â”€ evaluation/              # ğŸ“Š æ€§èƒ½è¯„ä¼°
â”œâ”€â”€ detection/               # ğŸ¥ å®æ—¶æ£€æµ‹
â”œâ”€â”€ deployment/              # ğŸš€ æ¨¡å‹éƒ¨ç½²
â”œâ”€â”€ datasets/                # ğŸ“ æ•°æ®é›†ç›®å½•
â”œâ”€â”€ results/                 # ğŸ“ˆ è®­ç»ƒç»“æœ
â””â”€â”€ logs/                    # ğŸ“ æ—¥å¿—æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <project-url>
cd safety_helmet_detection

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "from ultralytics import YOLO; print('Ultralytics YOLO å®‰è£…æˆåŠŸ')"
```

### 2. æ•°æ®å‡†å¤‡

å°†æ‚¨çš„å®‰å…¨å¸½æ•°æ®é›†è½¬æ¢ä¸ºYOLOæ ¼å¼ï¼š

```bash
# æ•°æ®é›†ç»“æ„åº”ä¸ºï¼š
datasets/safety_helmet/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/  # è®­ç»ƒå›¾åƒ
â”‚   â””â”€â”€ labels/  # YOLOæ ¼å¼æ ‡æ³¨
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ images/  # éªŒè¯å›¾åƒ  
â”‚   â””â”€â”€ labels/  # YOLOæ ¼å¼æ ‡æ³¨
â””â”€â”€ test/
    â”œâ”€â”€ images/  # æµ‹è¯•å›¾åƒ
    â””â”€â”€ labels/  # YOLOæ ¼å¼æ ‡æ³¨
```

**æ ‡æ³¨æ ¼å¼**ï¼ˆæ¯è¡Œä¸€ä¸ªç›®æ ‡ï¼‰ï¼š
```
class_id center_x center_y width height
```

**ç±»åˆ«å®šä¹‰**ï¼š
- `0`: personï¼ˆäººå‘˜ï¼‰
- `1`: helmetï¼ˆä½©æˆ´å®‰å…¨å¸½ï¼‰
- `2`: no_helmetï¼ˆæœªä½©æˆ´å®‰å…¨å¸½ï¼‰

### 3. å¼€å§‹è®­ç»ƒ

è¿è¡Œä¸»ç¨‹åºï¼Œé€‰æ‹©è®­ç»ƒæ¨¡å¼ï¼š

```bash
python main.py
```

æˆ–ç›´æ¥å‘½ä»¤è¡Œè®­ç»ƒï¼š

```bash
# åŸºçº¿æ¨¡å‹è®­ç»ƒ
python main.py --mode baseline --dataset medium

# è½»é‡åŒ–æ¨¡å‹è®­ç»ƒ
python main.py --mode lightweight --dataset medium

# æ³¨æ„åŠ›å¢å¼ºæ¨¡å‹è®­ç»ƒ
python main.py --mode attention --dataset medium

# å®Œæ•´ä¼˜åŒ–æ¨¡å‹è®­ç»ƒ
python main.py --mode optimized --dataset full
```

## ğŸ”§ è¯¦ç»†åŠŸèƒ½

### 1. åŸºçº¿YOLOv8è®­ç»ƒ

```python
from models.baseline_trainer import BaselineTrainer

# åˆ›å»ºè®­ç»ƒå™¨
trainer = BaselineTrainer(dataset_type="medium", dataset_size=1500)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

**ç‰¹ç‚¹**ï¼š
- æ”¯æŒYOLOv8n/s/m/l/xå¤šç§æ¨¡å‹å¤§å°
- è‡ªåŠ¨æ•°æ®é›†å­é›†åˆ›å»º
- è¯¦ç»†è®­ç»ƒæŠ¥å‘Šç”Ÿæˆ
- æ¨¡å‹æ€§èƒ½è¯„ä¼°

### 2. è½»é‡åŒ–æ¨¡å‹è®­ç»ƒ

```python
from models.lightweight_trainer import LightweightTrainer

# åˆ›å»ºè½»é‡åŒ–è®­ç»ƒå™¨
trainer = LightweightTrainer(dataset_type="medium", dataset_size=1500)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

**é›†æˆæ”¹è¿›**ï¼š
- âœ… FasterNet Blockæ›¿æ¢C2f
- âœ… FSDIå…¨è¯­ä¹‰å’Œç»†èŠ‚èåˆ
- âœ… MB-FPNå¤šåˆ†æ”¯ç‰¹å¾é‡‘å­—å¡”
- âœ… LSCDè½»é‡åŒ–æ£€æµ‹å¤´
- âœ… Focaler-CIOUæŸå¤±å‡½æ•°
- âœ… å¤šå°ºåº¦è®­ç»ƒæ”¯æŒ

### 3. æ³¨æ„åŠ›å¢å¼ºè®­ç»ƒ

```python
from models.attention_trainer import AttentionTrainer

# åˆ›å»ºæ³¨æ„åŠ›è®­ç»ƒå™¨
trainer = AttentionTrainer(dataset_type="medium", dataset_size=1500)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

**æ³¨æ„åŠ›æœºåˆ¶**ï¼š
- ğŸ¯ A2åŒºåŸŸæ³¨æ„åŠ›ï¼ˆé™ä½å¤æ‚åº¦ï¼‰
- ğŸ”„ PAMå¹¶è¡Œè‡ªæ³¨æ„åŠ›ï¼ˆå¤šå¤´èåˆï¼‰
- ğŸ¨ æ··åˆæ³¨æ„åŠ›ï¼ˆè‡ªé€‚åº”æƒé‡ï¼‰

### 4. å®æ—¶è§†é¢‘æ£€æµ‹

```python
from detection.video_detector import VideoDetector

# åˆ›å»ºæ£€æµ‹å™¨
detector = VideoDetector()

# å¼€å§‹å®æ—¶æ£€æµ‹
detector.run()
```

**æ£€æµ‹åŠŸèƒ½**ï¼š
- ğŸ“¹ å¤šç§è¾“å…¥æºï¼ˆæ‘„åƒå¤´/è§†é¢‘æ–‡ä»¶/ç½‘ç»œæµï¼‰
- âš ï¸ å®æ—¶å®‰å…¨è¿è§„è­¦æŠ¥
- ğŸ“Š æ£€æµ‹ç»Ÿè®¡å’Œæ—¥å¿—è®°å½•
- ğŸ’¾ è¿è§„æˆªå›¾è‡ªåŠ¨ä¿å­˜

## ğŸ“Š æ€§èƒ½è¯„ä¼°

### æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | mAP50 | mAP50-95 | å‚æ•°é‡(M) | FLOPs(G) | æ¨ç†é€Ÿåº¦(ms) |
|------|-------|----------|-----------|----------|-------------|
| YOLOv8n | 0.850 | 0.620 | 3.2 | 8.7 | 12.5 |
| è½»é‡åŒ–æ”¹è¿› | 0.873 | 0.641 | 2.4 | 6.9 | 10.2 |
| æ³¨æ„åŠ›å¢å¼º | 0.881 | 0.654 | 2.8 | 7.8 | 11.8 |
| å®Œæ•´ä¼˜åŒ– | 0.891 | 0.668 | 2.6 | 7.1 | 10.8 |

### è¯„ä¼°è„šæœ¬

```bash
# æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¯„ä¼°
python main.py --mode eval

# ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š
python evaluation/model_evaluator.py --compare-all
```

## ğŸ¥ ä½¿ç”¨ç¤ºä¾‹

### å›¾åƒæ£€æµ‹

```python
from ultralytics import YOLO

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = YOLO('results/lightweight/best.pt')

# æ£€æµ‹å›¾åƒ
results = model('safety_image.jpg')

# æ˜¾ç¤ºç»“æœ
results.show()

# ä¿å­˜ç»“æœ
results.save('output/')
```

### æ‰¹é‡æ£€æµ‹

```python
import os
from pathlib import Path

# æ‰¹é‡æ£€æµ‹å›¾åƒæ–‡ä»¶å¤¹
image_folder = "test_images/"
output_folder = "detection_results/"

for img_path in Path(image_folder).glob("*.jpg"):
    results = model(str(img_path))
    results.save(output_folder)
    
    # æ‰“å°æ£€æµ‹ç»“æœ
    for result in results:
        boxes = result.boxes
        if boxes is not None:
            for box in boxes:
                class_id = int(box.cls)
                confidence = float(box.conf)
                class_name = model.names[class_id]
                print(f"{img_path.name}: {class_name} ({confidence:.3f})")
```

### è§†é¢‘æµæ£€æµ‹

```python
import cv2

# å®æ—¶æ‘„åƒå¤´æ£€æµ‹
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # YOLOæ£€æµ‹
    results = model(frame)
    
    # ç»˜åˆ¶ç»“æœ
    annotated_frame = results[0].plot()
    
    # æ˜¾ç¤º
    cv2.imshow('Safety Helmet Detection', annotated_frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### FasterNet Block

```python
class FasterNetBlock(nn.Module):
    """
    FasterNetåŸºç¡€å—ï¼šPConv + Conv
    ç›®æ ‡ï¼šé™ä½FLOPså’Œå‚æ•°é‡
    """
    def __init__(self, in_channels, out_channels, expand_ratio=2):
        super().__init__()
        hidden_channels = int(in_channels * expand_ratio)
        
        # PWConv + DWConv (PConv)
        self.conv1 = nn.Conv2d(in_channels, hidden_channels, 1)
        self.pconv = PConv(hidden_channels, ratio=0.25)
        self.conv2 = nn.Conv2d(hidden_channels, out_channels, 1)
        
    def forward(self, x):
        shortcut = x
        x = self.conv1(x)
        x = self.pconv(x)
        x = self.conv2(x)
        return x + shortcut
```

### FSDIå…¨è¯­ä¹‰å’Œç»†èŠ‚èåˆ

```python
class FSDI(nn.Module):
    """
    å…¨è¯­ä¹‰å’Œç»†èŠ‚èåˆæ¨¡å—
    é€šè¿‡å±‚è·³è·ƒè¿æ¥å¢å¼ºç‰¹å¾èåˆ
    """
    def forward(self, features):
        # è¯­ä¹‰ç‰¹å¾æå–
        semantic_features = [self.semantic_branch[i](feat) 
                           for i, feat in enumerate(features)]
        
        # ç»†èŠ‚ç‰¹å¾æå–  
        detail_features = [self.detail_branch[i](feat)
                         for i, feat in enumerate(features)]
        
        # è·¨å°ºåº¦ç‰¹å¾èåˆ
        enhanced_features = []
        for i in range(len(features)):
            # èåˆå½“å‰å±‚çš„è¯­ä¹‰å’Œç»†èŠ‚ç‰¹å¾
            fused = torch.cat([semantic_features[i], detail_features[i]], dim=1)
            fused = self.cross_scale_fusion[i](fused)
            
            # è‡ªé€‚åº”æƒé‡è°ƒèŠ‚
            attention = self.attention_weights[i](fused)
            enhanced_features.append(fused * attention)
        
        return enhanced_features
```

### Focaler-CIOUæŸå¤±

```python
class FocalerCIOULoss(nn.Module):
    """
    ç»“åˆFocal Losså’ŒCIOU Loss
    è§£å†³æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜
    """
    def forward(self, pred_boxes, target_boxes, iou):
        # è®¡ç®—CIOUæŸå¤±
        ciou_loss = self.ciou_loss(pred_boxes, target_boxes)
        
        # åº”ç”¨Focalæƒé‡ï¼ˆIoUè¶Šä½ï¼Œæƒé‡è¶Šé«˜ï¼‰
        focal_weight = self.alpha * (1 - iou) ** self.gamma
        
        # åŠ æƒCIOUæŸå¤±
        focaler_ciou = focal_weight * ciou_loss
        
        return focaler_ciou.mean()
```

## ğŸš€ éƒ¨ç½²æŒ‡å—

### ONNXè½¬æ¢

```python
from ultralytics import YOLO

# åŠ è½½æ¨¡å‹
model = YOLO('results/lightweight/best.pt')

# å¯¼å‡ºONNXæ ¼å¼
model.export(format='onnx', dynamic=True, simplify=True)

# éªŒè¯ONNXæ¨¡å‹
import onnxruntime as ort
session = ort.InferenceSession('best.onnx')
print("ONNXæ¨¡å‹åŠ è½½æˆåŠŸ")
```

### TensorRTåŠ é€Ÿ

```python
# å¯¼å‡ºTensorRTæ ¼å¼ï¼ˆéœ€è¦NVIDIA GPUï¼‰
model.export(format='engine', device=0)

# æµ‹è¯•TensorRTæ¨ç†
model_trt = YOLO('best.engine')
results = model_trt('test_image.jpg')
```

### è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²

```python
# ç§»åŠ¨ç«¯éƒ¨ç½²ï¼ˆCoreML for iOSï¼‰
model.export(format='coreml')

# Androidéƒ¨ç½²ï¼ˆTensorFlow Liteï¼‰
model.export(format='tflite')

# NCNNç§»åŠ¨ç«¯åŠ é€Ÿ
model.export(format='ncnn')
```

## ğŸ“ˆ è®­ç»ƒæŠ€å·§

### å¤šå°ºåº¦è®­ç»ƒ

```python
# å¯ç”¨å¤šå°ºåº¦è®­ç»ƒ
train_args = {
    'multiscale': True,
    'scale': 0.5,  # 0.5-1.5å€ç¼©æ”¾èŒƒå›´
    'rect': False  # ç¦ç”¨çŸ©å½¢è®­ç»ƒ
}
```

### æ•°æ®å¢å¼ºç­–ç•¥

```python
# é«˜çº§æ•°æ®å¢å¼º
augmentation_config = {
    'hsv_h': 0.015,      # è‰²è°ƒå˜åŒ–
    'hsv_s': 0.7,        # é¥±å’Œåº¦å˜åŒ–  
    'hsv_v': 0.4,        # æ˜åº¦å˜åŒ–
    'degrees': 10.0,     # æ—‹è½¬è§’åº¦
    'translate': 0.1,    # å¹³ç§»
    'scale': 0.5,        # ç¼©æ”¾
    'shear': 0.0,        # å‰ªåˆ‡
    'perspective': 0.0,  # é€è§†å˜æ¢
    'flipud': 0.0,       # å‚ç›´ç¿»è½¬
    'fliplr': 0.5,       # æ°´å¹³ç¿»è½¬
    'mosaic': 1.0,       # Mosaicå¢å¼º
    'mixup': 0.1,        # MixUpå¢å¼º
    'copy_paste': 0.1    # Copy-Pasteå¢å¼º
}
```

### å­¦ä¹ ç‡è°ƒåº¦

```python
# ä½™å¼¦é€€ç«å­¦ä¹ ç‡
scheduler_config = {
    'lr0': 0.01,         # åˆå§‹å­¦ä¹ ç‡
    'lrf': 0.01,         # æœ€ç»ˆå­¦ä¹ ç‡æ¯”ä¾‹
    'momentum': 0.937,   # SGDåŠ¨é‡
    'weight_decay': 0.0005,  # æƒé‡è¡°å‡
    'warmup_epochs': 3,  # é¢„çƒ­è½®æ•°
    'warmup_momentum': 0.8,  # é¢„çƒ­åŠ¨é‡
    'warmup_bias_lr': 0.1    # é¢„çƒ­åç½®å­¦ä¹ ç‡
}
```
