"""
æ•°æ®é›†å¤„ç†å·¥å…·å‡½æ•°
æä¾›æ•°æ®é›†åˆ›å»ºã€å­é›†åˆ†å‰²ã€å¢å¼ºç­‰åŠŸèƒ½
"""
import os
import shutil
import random
import yaml
from pathlib import Path
from typing import Tuple, List
import numpy as np

def create_dataset_subset(source_dir: Path, target_dir: Path, 
                         train_size: int, val_size: int, test_size: int):
    """
    åˆ›å»ºæ•°æ®é›†å­é›†
    
    Args:
        source_dir: æºæ•°æ®é›†ç›®å½•
        target_dir: ç›®æ ‡æ•°æ®é›†ç›®å½•
        train_size: è®­ç»ƒé›†å¤§å°
        val_size: éªŒè¯é›†å¤§å°
        test_size: æµ‹è¯•é›†å¤§å°
    """
    print(f"åˆ›å»ºæ•°æ®é›†å­é›†: {target_dir}")
    
    # åˆ›å»ºç›®æ ‡ç›®å½•ç»“æ„
    for split in ['train', 'val', 'test']:
        for subdir in ['images', 'labels']:
            (target_dir / split / subdir).mkdir(parents=True, exist_ok=True)
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    source_images = []
    for ext in ['*.jpg', '*.jpeg', '*.png']:
        source_images.extend((source_dir / "train" / "images").glob(ext))
    
    if not source_images:
        print("âŒ æºæ•°æ®é›†ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶")
        return False
    
    # éšæœºé‡‡æ ·
    random.shuffle(source_images)
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ·æœ¬
    total_needed = train_size + val_size + test_size
    if len(source_images) < total_needed:
        print(f"âš ï¸ æºæ•°æ®é›†åªæœ‰{len(source_images)}å¼ å›¾åƒï¼Œéœ€è¦{total_needed}å¼ ")
        # è°ƒæ•´æ ·æœ¬åˆ†é…
        ratio = len(source_images) / total_needed
        train_size = int(train_size * ratio)
        val_size = int(val_size * ratio)
        test_size = len(source_images) - train_size - val_size
    
    # åˆ†é…æ ·æœ¬
    train_samples = source_images[:train_size]
    val_samples = source_images[train_size:train_size + val_size]
    test_samples = source_images[train_size + val_size:train_size + val_size + test_size]
    
    # å¤åˆ¶æ–‡ä»¶
    for samples, split in [(train_samples, 'train'), (val_samples, 'val'), (test_samples, 'test')]:
        for img_path in samples:
            try:
                # å¤åˆ¶å›¾åƒ
                shutil.copy2(img_path, target_dir / split / "images" / img_path.name)
                
                # å¤åˆ¶å¯¹åº”çš„æ ‡ç­¾
                label_name = img_path.stem + ".txt"
                label_path = source_dir / "train" / "labels" / label_name
                if label_path.exists():
                    shutil.copy2(label_path, target_dir / split / "labels" / label_name)
                else:
                    print(f"âš ï¸ æ‰¾ä¸åˆ°æ ‡ç­¾æ–‡ä»¶: {label_name}")
                    
            except Exception as e:
                print(f"âŒ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {img_path.name}: {e}")
    
    print(f"âœ… æ•°æ®é›†å­é›†åˆ›å»ºå®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_samples)} å¼ ")
    print(f"   éªŒè¯é›†: {len(val_samples)} å¼ ") 
    print(f"   æµ‹è¯•é›†: {len(test_samples)} å¼ ")
    
    return True

def analyze_dataset(dataset_dir: Path) -> dict:
    """
    åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        dataset_dir: æ•°æ®é›†ç›®å½•
        
    Returns:
        æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    stats = {
        'splits': {},
        'class_distribution': {},
        'bbox_stats': {}
    }
    
    class_names = ['person', 'helmet', 'no_helmet']
    
    for split in ['train', 'val', 'test']:
        split_dir = dataset_dir / split
        if not split_dir.exists():
            continue
            
        images_dir = split_dir / "images"
        labels_dir = split_dir / "labels"
        
        # ç»Ÿè®¡å›¾åƒæ•°é‡
        image_files = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png"))
        label_files = list(labels_dir.glob("*.txt"))
        
        # ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
        class_counts = [0] * len(class_names)
        bbox_sizes = []
        
        for label_file in label_files:
            try:
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            class_id = int(parts[0])
                            if 0 <= class_id < len(class_names):
                                class_counts[class_id] += 1
                            
                            # è®¡ç®—è¾¹ç•Œæ¡†å¤§å°
                            w, h = float(parts[3]), float(parts[4])
                            bbox_sizes.append(w * h)
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ ‡ç­¾æ–‡ä»¶å¤±è´¥ {label_file}: {e}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats['splits'][split] = {
            'images': len(image_files),
            'labels': len(label_files),
            'annotations': sum(class_counts)
        }
        
        stats['class_distribution'][split] = {
            class_names[i]: class_counts[i] for i in range(len(class_names))
        }
        
        if bbox_sizes:
            stats['bbox_stats'][split] = {
                'mean_size': np.mean(bbox_sizes),
                'std_size': np.std(bbox_sizes),
                'min_size': np.min(bbox_sizes),
                'max_size': np.max(bbox_sizes)
            }
    
    return stats

def print_dataset_stats(stats: dict):
    """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
    print("="*50)
    
    # æ•°æ®é›†åˆ†å‰²ç»Ÿè®¡
    print("æ•°æ®é›†åˆ†å‰²:")
    for split, info in stats['splits'].items():
        print(f"  {split:>5}: {info['images']:>4} å¼ å›¾åƒ, {info['annotations']:>5} ä¸ªæ ‡æ³¨")
    
    # ç±»åˆ«åˆ†å¸ƒ
    print("\nç±»åˆ«åˆ†å¸ƒ:")
    class_names = ['person', 'helmet', 'no_helmet']
    for split in ['train', 'val', 'test']:
        if split in stats['class_distribution']:
            print(f"  {split}:")
            for class_name in class_names:
                count = stats['class_distribution'][split].get(class_name, 0)
                print(f"    {class_name:>12}: {count:>5}")
    
    # è¾¹ç•Œæ¡†ç»Ÿè®¡
    print("\nè¾¹ç•Œæ¡†å¤§å°ç»Ÿè®¡:")
    for split, bbox_stats in stats['bbox_stats'].items():
        print(f"  {split}:")
        print(f"    å¹³å‡å¤§å°: {bbox_stats['mean_size']:.4f}")
        print(f"    æ ‡å‡†å·®:   {bbox_stats['std_size']:.4f}")
        print(f"    æœ€å°:     {bbox_stats['min_size']:.4f}")
        print(f"    æœ€å¤§:     {bbox_stats['max_size']:.4f}")

def validate_dataset(dataset_dir: Path) -> bool:
    """
    éªŒè¯æ•°æ®é›†å®Œæ•´æ€§
    
    Args:
        dataset_dir: æ•°æ®é›†ç›®å½•
        
    Returns:
        éªŒè¯æ˜¯å¦é€šè¿‡
    """
    print(f"ğŸ” éªŒè¯æ•°æ®é›†: {dataset_dir}")
    
    required_structure = [
        "train/images",
        "train/labels", 
        "val/images",
        "val/labels",
        "test/images",
        "test/labels"
    ]
    
    # æ£€æŸ¥ç›®å½•ç»“æ„
    for structure in required_structure:
        path = dataset_dir / structure
        if not path.exists():
            print(f"âŒ ç¼ºå°‘ç›®å½•: {structure}")
            return False
    
    # æ£€æŸ¥å›¾åƒå’Œæ ‡ç­¾å¯¹åº”å…³ç³»
    issues = []
    for split in ['train', 'val', 'test']:
        images_dir = dataset_dir / split / "images"
        labels_dir = dataset_dir / split / "labels"
        
        image_files = set(f.stem for f in images_dir.glob("*.jpg")) | \
                     set(f.stem for f in images_dir.glob("*.png"))
        label_files = set(f.stem for f in labels_dir.glob("*.txt"))
        
        # æ£€æŸ¥ç¼ºå¤±çš„æ ‡ç­¾
        missing_labels = image_files - label_files
        if missing_labels:
            issues.append(f"{split}åˆ†å‰²ç¼ºå°‘{len(missing_labels)}ä¸ªæ ‡ç­¾æ–‡ä»¶")
        
        # æ£€æŸ¥å¤šä½™çš„æ ‡ç­¾
        extra_labels = label_files - image_files
        if extra_labels:
            issues.append(f"{split}åˆ†å‰²æœ‰{len(extra_labels)}ä¸ªå¤šä½™æ ‡ç­¾æ–‡ä»¶")
    
    if issues:
        print("âš ï¸ æ•°æ®é›†éªŒè¯å‘ç°é—®é¢˜:")
        for issue in issues:
            print(f"   - {issue}")
        return False
    
    print("âœ… æ•°æ®é›†éªŒè¯é€šè¿‡")
    return True

def create_dataset_config(dataset_dir: Path, config_path: Path = None) -> str:
    """
    åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶
    
    Args:
        dataset_dir: æ•°æ®é›†ç›®å½•
        config_path: é…ç½®æ–‡ä»¶ä¿å­˜è·¯å¾„
        
    Returns:
        é…ç½®æ–‡ä»¶è·¯å¾„
    """
    if config_path is None:
        config_path = dataset_dir.parent / f"{dataset_dir.name}.yaml"
    
    config_data = {
        'train': str(dataset_dir / "train" / "images"),
        'val': str(dataset_dir / "val" / "images"),
        'test': str(dataset_dir / "test" / "images"),
        'nc': 3,
        'names': ['person', 'helmet', 'no_helmet']
    }
    
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True)
    
    print(f"âœ… æ•°æ®é›†é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
    return str(config_path)

def balance_dataset(dataset_dir: Path, target_dir: Path = None, 
                   min_samples_per_class: int = 100):
    """
    å¹³è¡¡æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ
    
    Args:
        dataset_dir: æºæ•°æ®é›†ç›®å½•
        target_dir: ç›®æ ‡æ•°æ®é›†ç›®å½•
        min_samples_per_class: æ¯ä¸ªç±»åˆ«æœ€å°‘æ ·æœ¬æ•°
    """
    if target_dir is None:
        target_dir = dataset_dir.parent / f"{dataset_dir.name}_balanced"
    
    print(f"ğŸ”„ å¹³è¡¡æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ...")
    
    # åˆ†æåŸå§‹æ•°æ®é›†
    stats = analyze_dataset(dataset_dir)
    
    # å®ç°æ•°æ®å¹³è¡¡é€»è¾‘
    # è¿™é‡Œå¯ä»¥é€šè¿‡å¤åˆ¶å°‘æ•°ç±»æ ·æœ¬æˆ–æ•°æ®å¢å¼ºæ¥å¹³è¡¡æ•°æ®é›†
    print("âš ï¸ æ•°æ®é›†å¹³è¡¡åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")

def augment_small_objects(dataset_dir: Path, target_dir: Path = None, 
                         small_threshold: float = 0.1):
    """
    å¯¹å°ç›®æ ‡è¿›è¡Œæ•°æ®å¢å¼º
    
    Args:
        dataset_dir: æºæ•°æ®é›†ç›®å½•
        target_dir: ç›®æ ‡æ•°æ®é›†ç›®å½•
        small_threshold: å°ç›®æ ‡é˜ˆå€¼ï¼ˆç›¸å¯¹äºå›¾åƒå¤§å°ï¼‰
    """
    if target_dir is None:
        target_dir = dataset_dir.parent / f"{dataset_dir.name}_augmented"
    
    print(f"ğŸ” å¯¹å°ç›®æ ‡è¿›è¡Œæ•°æ®å¢å¼º...")
    print("âš ï¸ å°ç›®æ ‡å¢å¼ºåŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    print("æµ‹è¯•æ•°æ®é›†å·¥å…·å‡½æ•°...")
    
    # æ¨¡æ‹Ÿæµ‹è¯•
    test_dir = Path("test_dataset")
    
    # æµ‹è¯•æ•°æ®é›†åˆ†æåŠŸèƒ½çš„é€»è¾‘
    print("âœ… æ•°æ®é›†å·¥å…·å‡½æ•°æµ‹è¯•å®Œæˆ") 