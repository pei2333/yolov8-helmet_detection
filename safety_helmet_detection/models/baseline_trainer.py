import os
import sys
import yaml
import torch
import random
import numpy as np
from pathlib import Path
from ultralytics import YOLO
import matplotlib.pyplot as plt
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

class BaselineTrainer:
    def __init__(self, dataset_type="medium", dataset_size=1500):
        self.dataset_type = dataset_type
        self.dataset_size = dataset_size
        self.project_root = Path(__file__).parent.parent
        
        self.results_dir = self.project_root / "results" / "baseline"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.config = {
            "model_size": "yolov8n",
            "epochs": 100,
            "batch_size": 16,
            "imgsz": 640,
            "lr0": 0.01,
            "weight_decay": 0.0005,
            "momentum": 0.937,
            "warmup_epochs": 3,
            "patience": 50,
            "save_period": 10,
            "device": "0" if torch.cuda.is_available() else "cpu"
        }
        
        self._set_seed(42)
        
    def _set_seed(self, seed):
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
    
    def prepare_dataset(self):
        print(f"ğŸ“Š å‡†å¤‡{self.dataset_type}æ•°æ®é›† ({self.dataset_size}å¼ å›¾åƒ)...")
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        dataset_dir = self.project_root / "datasets" / "safety_helmet"
        if not dataset_dir.exists():
            print("âŒ æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®é›†è½¬æ¢è„šæœ¬")
            return False
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹åˆ›å»ºå­é›†
        if self.dataset_type in ["subset", "medium"]:
            return self._create_dataset_subset()
        
        return True
    
    def _create_dataset_subset(self):
        """åˆ›å»ºæ•°æ®é›†å­é›†"""
        from utils.dataset_utils import create_dataset_subset
        
        try:
            subset_dir = self.project_root / "datasets" / f"safety_helmet_{self.dataset_type}"
            
            if not subset_dir.exists():
                print(f"åˆ›å»º{self.dataset_type}æ•°æ®é›†å­é›†...")
                create_dataset_subset(
                    source_dir=self.project_root / "datasets" / "safety_helmet",
                    target_dir=subset_dir,
                    train_size=int(self.dataset_size * 0.7),
                    val_size=int(self.dataset_size * 0.2),
                    test_size=int(self.dataset_size * 0.1)
                )
            
            # æ›´æ–°é…ç½®æ–‡ä»¶è·¯å¾„
            self._update_config_file(subset_dir)
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ•°æ®é›†å­é›†å¤±è´¥: {e}")
            return False
    
    def _update_config_file(self, dataset_dir):
        """æ›´æ–°æ•°æ®é›†é…ç½®æ–‡ä»¶"""
        config_path = self.project_root / "configs" / f"safety_helmet_{self.dataset_type}.yaml"
        
        config_data = {
            'train': str(dataset_dir / "train" / "images"),
            'val': str(dataset_dir / "val" / "images"),
            'test': str(dataset_dir / "test" / "images"),
            'nc': 3,
            'names': ['person', 'helmet', 'no_helmet']
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True)
        
        self.data_config = str(config_path)
    
    def select_model_size(self):
        print("\nğŸ”§ é€‰æ‹©YOLOv8æ¨¡å‹å¤§å°:")
        print("1. YOLOv8n (Nano) - æœ€å¿«ï¼Œå‚æ•°æœ€å°‘")
        print("2. YOLOv8s (Small) - å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦")
        print("3. YOLOv8m (Medium) - è¾ƒé«˜ç²¾åº¦")
        print("4. YOLOv8l (Large) - é«˜ç²¾åº¦")
        print("5. YOLOv8x (XLarge) - æœ€é«˜ç²¾åº¦")
        
        while True:
            choice = input("è¯·é€‰æ‹©æ¨¡å‹å¤§å° (1-5ï¼Œé»˜è®¤1): ").strip() or "1"
            
            if choice == "1":
                self.config["model_size"] = "yolov8n"
                break
            elif choice == "2":
                self.config["model_size"] = "yolov8s"
                break
            elif choice == "3":
                self.config["model_size"] = "yolov8m"
                break
            elif choice == "4":
                self.config["model_size"] = "yolov8l"
                break
            elif choice == "5":
                self.config["model_size"] = "yolov8x"
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    def configure_training(self):
        """é…ç½®è®­ç»ƒå‚æ•°"""
        print(f"\nâš™ï¸ é…ç½®è®­ç»ƒå‚æ•° (å½“å‰: {self.config['model_size']})...")
        
        # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´é»˜è®¤å‚æ•°
        if self.config["model_size"] in ["yolov8l", "yolov8x"]:
            self.config["batch_size"] = 8  # å¤§æ¨¡å‹å‡å°‘batch size
            self.config["epochs"] = 150   # å¤§æ¨¡å‹å¢åŠ è®­ç»ƒè½®æ•°
        elif self.config["model_size"] == "yolov8n":
            self.config["batch_size"] = 32  # å°æ¨¡å‹å¢åŠ batch size
            self.config["epochs"] = 200    # å°æ¨¡å‹éœ€è¦æ›´å¤šè½®æ•°
        
        # æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´å‚æ•°
        if self.dataset_type == "subset":
            self.config["epochs"] = 50
            self.config["patience"] = 20
        elif self.dataset_type == "full":
            self.config["epochs"] = 300
            self.config["patience"] = 100
        
        # æ˜¾ç¤ºå½“å‰é…ç½®
        print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
        print(f"   æ¨¡å‹: {self.config['model_size']}")
        print(f"   è½®æ•°: {self.config['epochs']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.config['batch_size']}")
        print(f"   å­¦ä¹ ç‡: {self.config['lr0']}")
        print(f"   å›¾åƒå¤§å°: {self.config['imgsz']}")
        print(f"   è®¾å¤‡: {self.config['device']}")
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        print("\nğŸš€ å¼€å§‹åŸºçº¿YOLOv8æ¨¡å‹è®­ç»ƒ...")
        
        # 1. å‡†å¤‡æ•°æ®é›†
        if not self.prepare_dataset():
            return False
        
        # 2. é€‰æ‹©æ¨¡å‹
        self.select_model_size()
        
        # 3. é…ç½®è®­ç»ƒå‚æ•°
        self.configure_training()
        
        # 4. åˆå§‹åŒ–æ¨¡å‹
        try:
            model = YOLO(f"{self.config['model_size']}.pt")
            print(f"âœ… æˆåŠŸåŠ è½½ {self.config['model_size']} é¢„è®­ç»ƒæ¨¡å‹")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
        
        # 5. å¼€å§‹è®­ç»ƒ
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            project_name = f"baseline_{self.config['model_size']}_{self.dataset_type}"
            
            results = model.train(
                data=self.data_config,
                epochs=self.config["epochs"],
                batch=self.config["batch_size"],
                imgsz=self.config["imgsz"],
                lr0=self.config["lr0"],
                weight_decay=self.config["weight_decay"],
                momentum=self.config["momentum"],
                warmup_epochs=self.config["warmup_epochs"],
                patience=self.config["patience"],
                save_period=self.config["save_period"],
                project=str(self.results_dir),
                name=f"{project_name}_{timestamp}",
                device=self.config["device"],
                verbose=True,
                plots=True
            )
            
            print(f"âœ… è®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.results_dir}")
            
            # 6. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
            self._generate_training_report(results, project_name, timestamp)
            
            return True
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _generate_training_report(self, results, project_name, timestamp):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        
        try:
            # åˆ›å»ºæŠ¥å‘Šç›®å½•
            report_dir = self.results_dir / f"{project_name}_{timestamp}"
            
            # è·å–è®­ç»ƒæŒ‡æ ‡
            metrics = results.results_dict if hasattr(results, 'results_dict') else {}
            
            # ç”ŸæˆMarkdownæŠ¥å‘Š
            report_content = f"""# åŸºçº¿YOLOv8è®­ç»ƒæŠ¥å‘Š
            
## è®­ç»ƒé…ç½®
- **æ¨¡å‹**: {self.config['model_size']}
- **æ•°æ®é›†**: {self.dataset_type} ({self.dataset_size}å¼ å›¾åƒ)
- **è®­ç»ƒè½®æ•°**: {self.config['epochs']}
- **æ‰¹æ¬¡å¤§å°**: {self.config['batch_size']}
- **å­¦ä¹ ç‡**: {self.config['lr0']}
- **è®¾å¤‡**: {self.config['device']}
- **è®­ç»ƒæ—¶é—´**: {timestamp}

## è®­ç»ƒç»“æœ
- **æœ€ç»ˆmAP50**: {metrics.get('metrics/mAP50(B)', 'N/A')}
- **æœ€ç»ˆmAP50-95**: {metrics.get('metrics/mAP50-95(B)', 'N/A')}
- **æœ€ä½³Precision**: {metrics.get('metrics/precision(B)', 'N/A')}
- **æœ€ä½³Recall**: {metrics.get('metrics/recall(B)', 'N/A')}

## æ–‡ä»¶ç»“æ„
```
{report_dir}/
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ best.pt          # æœ€ä½³æ¨¡å‹æƒé‡
â”‚   â””â”€â”€ last.pt          # æœ€åä¸€è½®æƒé‡
â”œâ”€â”€ results.png          # è®­ç»ƒæ›²çº¿å›¾
â”œâ”€â”€ confusion_matrix.png # æ··æ·†çŸ©é˜µ
â”œâ”€â”€ val_batch0_pred.jpg  # éªŒè¯ç»“æœç¤ºä¾‹
â””â”€â”€ train_batch0.jpg     # è®­ç»ƒæ‰¹æ¬¡ç¤ºä¾‹
```

## æ¨¡å‹ä½¿ç”¨
```python
from ultralytics import YOLO

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = YOLO('{report_dir}/weights/best.pt')

# è¿›è¡Œé¢„æµ‹
results = model('image.jpg')
```
"""
            
            # ä¿å­˜æŠ¥å‘Š
            report_path = report_dir / "training_report.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            print(f"ğŸ“‹ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
        except Exception as e:
            print(f"âš ï¸ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
    
    def evaluate_model(self, model_path=None):
        """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if model_path is None:
            # æŸ¥æ‰¾æœ€æ–°çš„æœ€ä½³æ¨¡å‹
            model_path = self._find_latest_model()
        
        if not model_path or not os.path.exists(model_path):
            print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶")
            return None
        
        try:
            model = YOLO(model_path)
            
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            results = model.val(data=self.data_config)
            
            print(f"ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
            print(f"   mAP50: {results.box.map50:.4f}")
            print(f"   mAP50-95: {results.box.map:.4f}")
            print(f"   Precision: {results.box.mp:.4f}")
            print(f"   Recall: {results.box.mr:.4f}")
            
            return results
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def _find_latest_model(self):
        """æŸ¥æ‰¾æœ€æ–°è®­ç»ƒçš„æ¨¡å‹"""
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒç»“æœç›®å½•
            result_dirs = [d for d in self.results_dir.iterdir() if d.is_dir()]
            if not result_dirs:
                return None
            
            latest_dir = max(result_dirs, key=lambda x: x.stat().st_mtime)
            model_path = latest_dir / "weights" / "best.pt"
            
            return str(model_path) if model_path.exists() else None
            
        except Exception:
            return None

# æ•°æ®é›†å·¥å…·å‡½æ•°
class DatasetUtils:
    """æ•°æ®é›†å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def create_dataset_subset(source_dir, target_dir, train_size, val_size, test_size):
        """åˆ›å»ºæ•°æ®é›†å­é›†"""
        import shutil
        
        # åˆ›å»ºç›®æ ‡ç›®å½•
        for split in ['train', 'val', 'test']:
            for subdir in ['images', 'labels']:
                (target_dir / split / subdir).mkdir(parents=True, exist_ok=True)
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        source_images = list((source_dir / "train" / "images").glob("*.jpg")) + \
                        list((source_dir / "train" / "images").glob("*.png"))
        
        # éšæœºé‡‡æ ·
        random.shuffle(source_images)
        
        # åˆ†é…æ ·æœ¬
        train_samples = source_images[:train_size]
        val_samples = source_images[train_size:train_size + val_size]
        test_samples = source_images[train_size + val_size:train_size + val_size + test_size]
        
        # å¤åˆ¶æ–‡ä»¶
        for samples, split in [(train_samples, 'train'), (val_samples, 'val'), (test_samples, 'test')]:
            for img_path in samples:
                # å¤åˆ¶å›¾åƒ
                shutil.copy2(img_path, target_dir / split / "images" / img_path.name)
                
                # å¤åˆ¶å¯¹åº”çš„æ ‡ç­¾
                label_name = img_path.stem + ".txt"
                label_path = source_dir / "train" / "labels" / label_name
                if label_path.exists():
                    shutil.copy2(label_path, target_dir / split / "labels" / label_name)
        
        print(f"âœ… æ•°æ®é›†å­é›†åˆ›å»ºå®Œæˆ: {target_dir}")
        print(f"   è®­ç»ƒé›†: {len(train_samples)} å¼ ")
        print(f"   éªŒè¯é›†: {len(val_samples)} å¼ ")
        print(f"   æµ‹è¯•é›†: {len(test_samples)} å¼ ")

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    print("æµ‹è¯•åŸºçº¿è®­ç»ƒå™¨...")
    
    # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
    trainer = BaselineTrainer(dataset_type="subset", dataset_size=100)
    
    # æµ‹è¯•é…ç½®
    trainer.select_model_size()
    trainer.configure_training()
    
    print("åŸºçº¿è®­ç»ƒå™¨æµ‹è¯•å®Œæˆ! âœ…") 